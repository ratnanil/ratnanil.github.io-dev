[
  {
    "objectID": "posts/2021-11-05-warping-switzerland-back-into-shape/stormy-transformations.html",
    "href": "posts/2021-11-05-warping-switzerland-back-into-shape/stormy-transformations.html",
    "title": "Warping Switzerland back into shape",
    "section": "",
    "text": "When I started working with swiss geodata a couple of years ago, Switzerland was beginning to transition from it’s old coordinate system CH1903 LV03 to CH1903+ LV95 (EPSG 2056).\nThe new coordinate system was awkward from the start. First off, I suddenly was un unable to convert data from WGS84 using a custom R script that swisstopo had previously provided for CH1903 LV03. The answer to my question on a swisstopo google group (in 2017) was to use swisstopo’s REST api.\nThis problem was seemingly solved with the emergence of sf and it’s built-in methods to reproject coordinates easily. Until one day I realized that the coordinate transformations to and from EPSG 2056 were fairly imprecise. The transformations did not account for the spatially varying offset for which the new coordinate system was implemented in the first place.\nI filed an issue on sf’s github repo beginning of 2020, but it wasn’t until recently (with PROJ 7.0.0) that I was able to precisely transform my data to and from EPSG 2056.\nThe issue is highlighted with the function list_coordOps() from rgdal. Apparently, the transformation was lacking a grid with the name ch_swisstopo_CHENyx06a.tif (the situation is now different, see my edit below).\nWith my current version of sf and rgdal, I need to download this grid manually and move it to the PROJ directory. Running list_coordOps now shows a different output.\nThe downloaded file is a so called transformation grid: A raster dataset in WGS84 containing information on the lat and lon offset for a given cell.\nI now can use this grid to precisely transform coordinates to and from ESPG 2056. I will demonstrate this by visualizing the directional offset, similar to the transformation grid I downloaded."
  },
  {
    "objectID": "posts/2021-11-05-warping-switzerland-back-into-shape/stormy-transformations.html#edit-2022-04-24",
    "href": "posts/2021-11-05-warping-switzerland-back-into-shape/stormy-transformations.html#edit-2022-04-24",
    "title": "Warping Switzerland back into shape",
    "section": "EDIT (2022-04-24):",
    "text": "EDIT (2022-04-24):\nAt the time of writing this blogpost, the bash command to download and move the file to the correct place was as follows:\nwget https://cdn.proj.org/ch_swisstopo_CHENyx06a.tif`\nsudo mv ch_swisstopo_CHENyx06a.tif /usr/share/proj/\nToday however, I needed to download a zip file and unzip it to /usr/share/proj/. I assume this will change again in the near future.\nhttps://download.osgeo.org/proj/proj-datumgrid-europe-1.5.zip \nsudo unzip proj-datumgrid-europe-1.5 /usr/share/proj/"
  },
  {
    "objectID": "posts/2021-10-22-benchmarking-binary-predicates/benchmarking_binary_predicates.html",
    "href": "posts/2021-10-22-benchmarking-binary-predicates/benchmarking_binary_predicates.html",
    "title": "Benchmarking binary predicates",
    "section": "",
    "text": "To make things more interesting, I won’t use my usual Swiss data for this test, but data from my second home, Sri Lanka. More specifically: I will use the Geonames data (> 50k points) and the administrative boundaries of Sri Lanka (26 polygons).\n\n\nloading libraries\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(sf)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(microbenchmark)\nlibrary(ggridges)\nlibrary(forcats)\n\n\n\n\npreparing boundary data\n# Downloaded from: https://data.humdata.org/dataset/sri-lanka-administrative-levels-0-4-boundaries\n# Administrative Level 0: country (1 features)\n# Administrative Level 1: province (9 features)\n# Administrative Level 2: district (26 features)\n# Administrative Level 3: divisional secretatiat (333 features)\n# Administrative Level 4: grama niladhari (14'044 features)\n\ntmp <- tempdir()\n\nboundary_dir <- file.path(tmp, \"boundary\")\nunzip(\"data-git-lfs/lka_adm_slsd_20200305_shp.zip\", exdir = boundary_dir)\n\nsl_boundary_l2 <- read_sf(\n  file.path(boundary_dir, \"lka_admbnda_adm2_slsd_20200305.shp\")\n  )\n# https://epsg.io/5234\n# https://epsg.io/5235\n\n\n\n\npreparing geonames datase\n# geonameid         : integer id of record in geonames database\n# name              : name of geographical point (utf8) varchar(200)\n# asciiname         : name of geographical point in plain ascii characters, varchar(200)\n# alternatenames    : alternatenames, comma separated, ascii names automatically transliterated, convenience attribute from alternatename table, varchar(10000)\n# latitude          : latitude in decimal degrees (wgs84)\n# longitude         : longitude in decimal degrees (wgs84)\n# feature class     : see http://www.geonames.org/export/codes.html, char(1)\n# feature code      : see http://www.geonames.org/export/codes.html, varchar(10)\n# country code      : ISO-3166 2-letter country code, 2 characters\n# cc2               : alternate country codes, comma separated, ISO-3166 2-letter country code, 200 characters\n# admin1 code       : fipscode (subject to change to iso code), see exceptions below, see file admin1Codes.txt for display names of this code; varchar(20)\n# admin2 code       : code for the second administrative division, a county in the US, see file admin2Codes.txt; varchar(80) \n# admin3 code       : code for third level administrative division, varchar(20)\n# admin4 code       : code for fourth level administrative division, varchar(20)\n# population        : bigint (8 byte int) \n# elevation         : in meters, integer\n# dem               : digital elevation model, srtm3 or gtopo30, average elevation of 3''x3'' (ca 90mx90m) or 30''x30'' (ca 900mx900m) area in meters, integer. srtm processed by cgiar/ciat.\n# timezone          : the iana timezone id (see file timeZone.txt) varchar(40)\n# modification date : date of last modification in yyyy-MM-dd format\n\n\ncolnames <- c(\"geonameid\", \"name\",  \"asciiname\",  \"alternatenames\", \"latitude\",  \n              \"longitude\",  \"feature_class\",  \"feature_code\", \"country_code\",  \n              \"cc2\",  \"admin1_code\",  \"admin2_code\",  \"admin3_code\",  \n              \"admin4_code\",  \"population\", \"elevation\", \"dem\", \"timezone\",  \n              \"modification_date\")\n\n\ngeonames_dir <- file.path(tmp, \"geonames\")\n\nunzip(\"data-git-lfs/LK.zip\", exdir = geonames_dir)\n\ngeonames <- read_tsv(file.path(geonames_dir, \"LK.txt\"),col_names = colnames) %>%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nRows: 56748 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr   (8): name, asciiname, alternatenames, feature_class, feature_code, cou...\ndbl  (10): geonameid, latitude, longitude, admin1_code, admin2_code, admin3_...\ndate  (1): modification_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOnce all the data is imported, I can demonstrate visually the task. I want to subset all points within the province of Kandy (incidentally where I spent 5 superb years of my childhood). Using st_within() for this operation, the output looks like this:\n\n\n\nsubsetting and creating a map\nkandy <- filter(sl_boundary_l2, ADM2_EN == \"Kandy\")\n\npoints_filter <- list(\n  within = geonames[st_within(geonames,kandy,sparse = FALSE)[,1],]\n)\n\nplot_bg_col <- Sys.getenv(\"plot_bg_col\")\ntext_col <- Sys.getenv(\"text_col\")\n\np1 <- ggplot(sl_boundary_l2) + \n  geom_sf(color = \"#ffffff\", fill = \"#ababab\") +\n  geom_sf(data = rbind(transmute(geonames, val = \"all points\"), \n                       transmute(points_filter[[\"within\"]], \n                                 val = \"points within the\\nprovince of Kandy\")), \n          alpha = 0.05, size = 0.05, color = \"#8d2663\") +\n  geom_sf(data = ~filter(., ADM2_EN == \"Kandy\"), fill = NA, color = \"#000000\") +\n  facet_wrap(~val) +\n  coord_sf(xlim = c(78, 83)) + \n  theme(strip.background = element_blank(),\n        strip.text = element_text(color = text_col),\n        panel.background = element_blank(),\n        plot.background = element_rect(fill = plot_bg_col),\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        )\n\np1\n\n\n\n\n\nNext, I will do the same operation with the other functions and also check the output number of rows to see if they are similar (they might be slightly off if we have points exactly on the polygon boundary) or even identical.\n\npoints_filter[[\"contains\"]] <- geonames[st_contains(kandy,\n                                                    geonames,\n                                                    sparse = FALSE)[1,],]\npoints_filter[[\"intersects\"]] <- geonames[st_intersects(geonames,\n                                                        kandy,\n                                                        sparse = FALSE)[,1],]\npoints_filter[[\"covered_by\"]] <- geonames[st_covered_by(geonames,\n                                                        kandy,\n                                                        sparse = FALSE)[,1],]\n\ntibble(\n  function_name = names(points_filter),\n  nrow = sapply(points_filter, nrow),\n  identical_to_st_within = sapply(points_filter, function(x){\n    identical(points_filter[[\"within\"]], x)\n    })\n) %>%\n  knitr::kable(col.names = stringr::str_replace_all(colnames(.),\"_\", \" \"))\n\n\n\n\nfunction name\nnrow\nidentical to st within\n\n\n\n\nwithin\n3251\nTRUE\n\n\ncontains\n3251\nTRUE\n\n\nintersects\n3251\nTRUE\n\n\ncovered_by\n3251\nTRUE\n\n\n\n\n\nTo find out which function is the fastest, I use the package microbenchmark. Since it doesn’t always take the same amount of time to process the same function, each function is executed multiple times (times = 50) and we will look at the distribution of the execution times.\n\n\nBenchmarking the functions\nmbm  <- microbenchmark(\n  intersects = st_intersects(kandy,geonames),\n  within = st_within(geonames,kandy),\n  contains = st_contains(kandy,geonames),\n  covered_by = st_covered_by(geonames,kandy),\n  times = 50\n)\n\n\n\n\n\n\n\n\n\n\nvisualizing the result\nmbm2df <- function(mbm_obj){\n  df <- as.data.frame(mbm_obj)\n  df$time <- dnanoseconds(df$time)\n  df\n}\n\nmbm_df <- mbm2df(mbm)\n\n\np2 <- mbm_df %>%\n  mutate(\n    expr = fct_reorder(expr,time,median,.desc = TRUE)\n  ) %>%\n  ggplot(aes(time,expr,fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +\n  scale_fill_viridis_c(option = \"C\")  +\n  scale_x_time(name = \"Duration (in seconds)\",\n               labels = scales::time_format(format = \"%OS3\")) +\n  labs(y = \"Function\") +\n  theme_minimal() +\n  theme(legend.position=\"none\",\n        plot.background = element_rect(fill = plot_bg_col), panel.grid.minor.x  = element_blank(),panel.grid.major.x = element_blank(),  axis.text = element_text(colour = text_col), text = element_text(colour = text_col)) \n\np2\n\n\nPicking joint bandwidth of 0.0204\n\n\n\n\n\nThis benchmark shows that st_contains and st_intersects executes faster than st_covered_by and st_within. The next question is: How do the functions scale and perform under different scenarios? I’ll test this by generating additional points to subset, and also by using more provinces than just Kandy. And since I’m more interested in relative rather than absolute execution times, I will calculate the median duration per function and scenario and rescale the values by deviding them with the duration of st_intersects.\n\n\nBenchmarking scalability\nn_points_vec <- c(100e3,200e3,500e3)\nn_poly_vec <- c(1,9,17,26)\n\nmbm2 <- map_dfr(n_points_vec,function(n_points){\n  \n  points <- st_sample(sl_boundary_l2,n_points,what = \"centers\")\n\n  mbm_points <- map_dfr(n_poly_vec, function(n_poly){\n    \n    polygons <- sample_n(sl_boundary_l2, n_poly)\n    \n    mbm_poly <- microbenchmark(\n      intersects = st_intersects(polygons,points),\n      within = st_within(points,polygons),\n      contains = st_contains(polygons,points),\n      covered_by = st_covered_by(points,polygons),\n      times = 10\n      )\n\n    as_tibble(mbm_poly) %>%\n      mutate(n_poly = n_poly)\n  }) %>%\n    mutate(n_points = n_points)\n})\n\n\n\n\n\n\n\n\n\n\nVisualizing results\nmbm2_df <- mbm2df(mbm2)\n\n\n\nmylabels <- function(x){sprintf(\"%+3.f%%\", x*100)}\n\nmbm2_df %>%\n  group_by(expr, n_poly, n_points) %>%\n  summarise(median = median(time)) %>% \n  ungroup() %>%\n  group_by(n_poly,n_points) %>%\n  mutate(\n    perc = median/median[expr == \"contains\"]-1,\n    expr = fct_relevel(expr, \"within\", \"covered_by\",\"intersects\", \"contains\")\n    ) %>%\n  ggplot(aes(perc,as.factor(expr), color = expr, fill = expr)) +\n  geom_point() +\n  geom_linerange(aes(xmin = 0, xmax = perc)) +\n  # expand_limits(x = 0) +\n  scale_x_continuous(\"Relative execution time (compared to 'st_contains')\", \n                     breaks = seq(-.4,.4,.2), labels = mylabels,\n                     limits = c(-.5,.5), \n                     sec.axis = sec_axis(~.x, \n                                         breaks = c(-.4,.4), \n                                         labels = c(\"< faster\",\"slower > \"))) +\n  labs(y = \"\") +\n  facet_grid(n_poly~n_points, \n             labeller = labeller(n_points = ~paste0(as.integer(.x)/1e3, \"K points\"),\n                                 n_poly = ~paste0(.x, \" polygons\")))+\n  # theme_light() +\n  theme(legend.position = \"none\", \n        axis.ticks.x.top = element_blank(), \n        text = element_text(size = 9),\n        plot.background = element_rect(fill = plot_bg_col),\n        panel.background = element_rect(fill = plot_bg_col),\n        panel.grid = element_blank(),axis.text = element_text(colour = text_col),axis.title = element_text(colour = text_col),strip.background = element_rect(fill = text_col)\n        )\n\n\n\n\n\n\n\nvisualizing the result\nmbm2_df %>%\n  group_by(expr, n_poly, n_points) %>%\n  summarise(median = median(time)) %>% \n  ungroup() %>%\n  ggplot(aes(n_poly,median, color = expr, fill = expr)) +\n  geom_point() +\n  geom_line()  +\n  scale_y_time(name = \"Duration (in seconds)\", \n               labels = scales::time_format(format = \"%OS2\")) +\n  scale_x_continuous(name = \"Number of Polygons\") +\n  facet_wrap(~n_points, \n             labeller = labeller(n_points = ~paste0(as.integer(.x)/1e3, \"K points\")), \n             scales = \"free_y\", ncol = 1) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nThis test shows something interesting: While st_contains and st_intersects are fast with a single polygon, they don’t scale well with lager number of polygons. This effect is especially prominent when the number of points is large (~500K).\nMy take home message from this whole exercise: If you want to subset points based on whether they lie in specific polygons or not, use st_intersects or st_contains when the number of polygons is small. Use st_covered_by or st_within when the number of polygons is large. If it’s important what happens to points lying on the edge, remember that only st_intersects and st_covered_by will include them.\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nespecially since I mostly don’t care what happens to point which lie exactly on the polygon edge↩︎\nst_within and st_contains will disregard points on a line, st_intersects and st_covered_by will include them↩︎"
  },
  {
    "objectID": "posts/2021-12-01-opendata-per-canton/opendata-per-canton.html",
    "href": "posts/2021-12-01-opendata-per-canton/opendata-per-canton.html",
    "title": "How open are our cantons?",
    "section": "",
    "text": "To be fair, the platform is amazing (although lacking an API) and most cantons are very accommodating, providing the data freely. Some cantons however, are still extremely restrictive with their publicly funded data, asking for close to 50’000 CHF (!!) for commercial use of their data! I’m lucky enough to be working in a research project at a university, but I can’t help but mourn the opportunities this data could be used for if it was provided freely.\nAnyway, this experience lead me to the question: Which cantons are most open in their data sharing policy? Which cantons are more restrictive? To answer this question, I scraped the title-page of each of the 23 datasets on geodienste.ch and visualized the results.\ngeodienste.ch provides three different methods to obtain the data. Either 1) the data is freely available without registration, 2) the data is freely available after registering on the website or 3) the canton needs to approve your request. This last method can mean that you will be grated approval within a few hours, or that you need to wait a couple of days to receive an email asking you to pay horrendous amounts (if you want to use it commercially).\nThe results show that most datasets available on the website are offered freely and without the need of registration. Some few datasets require registration, and only 6 cantons feel the need to manually approve and potentially charge certain datasets. Foremost, the cantons Jura, Ticino and Valais heavily guard their data and require approval on a large number of their datasets.\nMost cantons offer between 10 and 15 datasets on geodienste.ch. The canton Schwyz has the highest number of datasets online (20), while Zug, Bern and Schaffhausen share second place with 18 datasets each. All provide their data to unregistered users freely. Good for you, that is the way to go!\n\n\nshow code for webscraping\nlibrary(httr)\nlibrary(xml2)\nlibrary(rvest)\nlibrary(tidyverse)\n\nservices <- read_html(\"https://geodienste.ch/versions_overview\") %>%\n  html_elements(\"a\") %>%\n  html_attr(\"href\")\n\nservices <- services[str_detect(services, \"/services/\")]\n\n\nkantone <- read_html(\"https://geodienste.ch/services/av\") %>%\n  html_elements(\".wappen-kt\") %>%\n  html_text()\n\nkantone <- str_trim(kantone)\nwappen <- read_html(\"https://geodienste.ch/services/av\") %>%\n  html_elements(\".wappen-kt\") %>%\n  html_elements(\"img\") %>%\n  html_attr(\"src\")\n\nkantone2 <- paste0(kantone,str_extract(wappen, \"\\\\.\\\\w{3}$\"))\n\n\nmap2(kantone2, wappen, function(x,y){\n  download.file(paste0(\"https://geodienste.ch/\",y),file.path(\"wappen\",x))\n})\n\nfi <- list.files(\"wappen/\",\".png\", full.names = TRUE)\n\nfile.rename(fi, str_remove(fi, \" \"))\n\n\nmyres <- map(services, function(url_i){\n  res <- read_html(paste0(\"https://geodienste.ch/\",url_i)) %>%\n    # html_elements(\".canton-table\") %>%\n    html_table() %>%\n    magrittr::extract2(1)\n  \n  res <- res %>% \n    janitor::clean_names()\n  \n  res %>%\n    filter(!is.na(info))\n})\n\nres2 <- map2_dfr(myres, services, function(mydf, serv){\n  mydf <- mydf[,1:3]\n  mydf$services <- serv\n  mydf\n})\n\nwrite_csv(res2, \"geodienste-raw.csv\")\n\n\n\n\nshow code for data preparation\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(cowplot)\n\n\nres2 <- read_csv(\"geodienste-raw.csv\")\nfacs <- c(\"Frei erhältlich\",\"Registrierung erforderlich\",\"Freigabe erforderlich\",\"Im Aufbau\", \"keine Daten\")\n\nres3 <- res2 %>%\n  separate(verfugbarkeit, c(\"verfugbarkeit\",\"verfugbarkeit2\"), sep = \"\\\\n\\\\s+\") %>% \n  select(-verfugbarkeit2) %>%# verfugbarkeit2 seems to be erroneous \n  mutate(\n    erhaeltlich_ab = str_extract(verfugbarkeit, \"\\\\d{2}\\\\.\\\\d{2}\\\\.\\\\d{4}\"),\n    verfugbarkeit = str_remove(verfugbarkeit, \"\\\\s\\\\(.+\\\\)\")\n  ) %>%\n  rename(kanton = x)\n\n\nres3_wide <-res3 %>%\n  group_by(kanton, verfugbarkeit) %>%  \n  count() %>%\n  mutate(verfugbarkeit_code = paste0(\"verf\",as.integer(factor(verfugbarkeit, facs)))) %>%\n  ungroup() %>%\n  select(-verfugbarkeit) %>%\n  pivot_wider(names_from = verfugbarkeit_code, values_from = n,values_fill = 0) %>%\n  select(kanton, order(colnames(.))) %>%\n  arrange(across(starts_with(\"verf\")))\n\n\nres4 <- res3 %>%\n  mutate(\n    kanton = factor(kanton, levels = res3_wide$kanton, ordered = TRUE),\n    verfugbarkeit = factor(verfugbarkeit, levels = facs, ordered = TRUE),\n    x = 1\n  ) \n\n\nwappen_df <- tibble(file = list.files(\"wappen\",full.names = TRUE)) %>%\n  mutate(\n    kanton = str_extract(file, \"[A-Z][A-Z]\"),\n) %>%\n  left_join(res3_wide, ., by = \"kanton\") %>%\n  mutate(y = row_number())\n\n\n\n\nshow code for creating the plot\ncols <- rev(RColorBrewer::brewer.pal(5, \"RdYlGn\"))\n\nplot_bg_col <- Sys.getenv(\"plot_bg_col\") \ntext_col <- Sys.getenv(\"text_col\")\n\np <- res4 %>%\n  ggplot(aes(x, kanton, fill = verfugbarkeit)) + \n  geom_col(position = position_stack(reverse = TRUE), color = plot_bg_col) +\n  pmap(select(wappen_df, file, y), function(file, y){draw_image(file, x = -0, y = y, width = 0.8, height = 0.6,hjust = 1,vjust = 0.5)}) +\n  scale_fill_manual(\"Status\",values = cols) +\n  scale_x_continuous(\"Anzahl Datensätze\",sec.axis = sec_axis(~./23,\"Anteil der Datensätze\", labels = scales::percent_format())) +\n  # guides(fill = guide_legend(title.position = \"top\",title.hjust = 0.5)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        axis.title.y = element_blank(), \n        panel.grid = element_blank(),\n        plot.background = element_rect(fill = plot_bg_col, colour = NA),\n        panel.background = element_rect(fill = plot_bg_col, colour = NA),\n        legend.text = element_text(size = 7),\n        legend.title = element_blank(),\n        text = element_text(colour = text_col),\n        axis.text = element_text(colour = text_col)\n        ) +\n  coord_equal() #+ theme(plot.background = element_rect(fill = \"plot_bg_col\"))\n\nggsave(\"preview.png\", width = 15, height = 10, units = \"cm\",scale = 1.4)"
  },
  {
    "objectID": "posts/2021-08-13-minimalistic-topography/minimalistic-topography.html",
    "href": "posts/2021-08-13-minimalistic-topography/minimalistic-topography.html",
    "title": "Minimalistic topography",
    "section": "",
    "text": "The way I understand it, Carla drew “horizontal” (latitudanal) elevation profiles at equal intervals over the continent and filled these elevation profiles to visualize not only the continent’s topography, but also implicitly showing it’s borders.\nI found this a very nice approach and tried recreating this idea with R for my home country, Switzerland. I’m quite happy with the result, however there is still a lot of room for improvement. I’ve packed the approach into generic functions, see below for the complete source code. Check below to see the source code.\n\n\nCreate some generic functions\n\n#' Create ridgelines from a digital elevation model (dhm)\n#'\n#' dhm: path to a dhm that can be imported using terra::rast\n#' n_lines: how many lines / polygons do you want to draw? Default is 50\n#' vspace: vertical space between lines, in units provided by the dhm. This overrides n_lines\n#' fac: How much of the space between the lines should be occupied by the hightest elevation?\n#' point_density: Density of the point samples used to extract elevation. Defaults to the inverse of the raster resolution\n#' geom_type: What should the output geometry type be? Can be LINESTRING or POLYGON\ncreate_ridges <- function(dhm, n_lines = 50, vspace = NULL, fac = 2, point_density = NULL, geom_type = \"LINESTRING\"){\n  \n  library(sf)\n  library(terra)\n  library(purrr)\n  \n  # extract the extent of the dhm as a vector\n  ex <- ext(dhm) %>%\n    as.vector()\n  \n  # If vspace is NULL (default), then vspace is calculated using n_lines\n  if(is.null(vspace)){\n    vspace <- (ex[\"ymax\"] - ex[\"ymin\"])/n_lines\n  }\n  \n  \n  point_density <- if(is.null(point_density)){1/terra::res(dhm)[2]}\n  \n  # Defines at what y-coordinates elevation should be extracted\n  heights <- seq(ex[\"ymin\"], ex[\"ymax\"], vspace)\n  \n  # calculates the x/y coordinates to extract points from the dhm\n  mypoints_mat <- map(heights, function(height){\n    matrix(c(ex[\"xmin\"], height, ex[\"xmax\"], height), ncol = 2, byrow = TRUE) %>%\n      st_linestring()\n  }) %>%\n    st_as_sfc() %>%\n    st_line_sample(density = point_density,type = \"regular\") %>%\n    st_as_sf() %>%\n    st_cast(\"POINT\") %>%\n    st_coordinates()\n  \n  \n  # extracts the elevation from the dhm\n  extracted <- terra::extract(dhm, mypoints_mat) %>% \n    cbind(mypoints_mat) %>% \n    as_tibble()\n  \n  # calculates the factor with which to multiply elevation, based on \"fac\" and the maximum elevation value\n  fac <- vspace*fac/max(extracted[,1], na.rm = TRUE)\n  \n  # calculates the coordinats of the ridge lines\n  coords <-extracted %>%\n    filter(!is.na(extracted[,1])) %>%\n    split(.$Y) %>%\n    imap(function(df, hig){\n      hig <- as.numeric(hig)\n      Y_new <- hig+pull(df[,1])*fac\n      matrix(c(df$X, Y_new), ncol = 2)\n    })\n\n  # creates LINESTRING or POLYGON, based on the \"geom_type\"\n  geoms <- if(geom_type == \"LINESTRING\"){\n    map(coords, ~st_linestring(.x))\n  } else if(geom_type == \"POLYGON\"){\n    imap(coords, function(x, hig){\n      hig <- as.numeric(hig)\n      \n      first <- head(x, 1)\n      first[,2] <- hig\n      last <- tail(x, 1)\n      last[,2] <- hig\n      \n      st_polygon(list(rbind(first, x, last, first)))\n    })\n  } else{\n    stop(paste0(\"This geom_type is not implemented:\",geom_type,\". geom_type must be 'LINESTRING' or 'POLYGON'\"))\n  }\n  \n  # adds the CRS to the output sfc\n  dhm_crs <- crs(dhm)\n  \n  if(dhm_crs == \"\") warning(\"dhm does not seem to have a CRS, therefore the output does not have a CRS assigned either.\")\n  \n  geoms %>%\n    st_sfc() %>%\n    st_set_crs(dhm_crs)\n  \n}\n\n# A helper function to creteate a polygon from the extent of a (dhm) raster\nst_bbox_rast <- function(rast_obj){\n  \n  library(terra)\n  library(sf)\n  \n  ex <- ext(rast_obj) %>%\n    as.vector()\n  \n  matrix(c(ex[1],ex[3],ex[1], ex[4],ex[2], ex[4],ex[2],ex[3],ex[1],ex[3]),ncol = 2, byrow = TRUE) %>%\n  list() %>%\n  st_polygon() %>% \n    st_sfc(crs = crs(rast_obj))\n}\n\n\n\nImport data and use the functions\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\n# library(ragg)\n\n\ndhm <- terra::rast(\"data-git-lfs/DHM25/DHM200.asc\")\ncrs(dhm) <- \"epsg:21781\"\n\n\nswitzerland_21781 <- sf::read_sf(\"data-git-lfs/swissboundaries/swissBOUNDARIES3D_1_3_TLM_LANDESGEBIET.shp\") %>%\n  st_union() %>%\n  st_transform(21781) \n\nmymask <- st_bbox_rast(dhm) %>%\n  st_buffer(5000) %>%\n  st_difference(switzerland_21781)\n\n\n\n\n\nsf_obj <- create_ridges(dhm,n_lines = 35, fac = 1.1,geom_type = \"POLYGON\")\n\n# bg_color <- \"#27363B\"\nbg_color <- Sys.getenv(\"plot_bg_col\")\nfg_color <- \"#EB4960\"\nfamily <- \"FreeMono\"\n\n\n\n\nbbox_switz <- st_bbox(switzerland_21781)\nbbox_switz_enlarge <- st_buffer(st_as_sfc(bbox_switz),50000)\nlims <- st_bbox(bbox_switz_enlarge)\nxlims =  lims[c(\"xmin\",\"xmax\")]\nylims = lims[c(\"ymin\",\"ymax\")]\n\nasp <- diff(ylims)/diff(xlims)\n\n\n\n\nCode\nmyplot <- ggplot(sf_obj) +\n  geom_sf(color = \"NA\", fill = fg_color)  + \n  geom_sf(data = mymask, color = \"NA\", fill = bg_color) +\n  # geom_sf(data = bbox_switz_enlarge, fill = \"NA\") +\n  ggtext::geom_richtext(aes(x = median(xlims), y = quantile(ylims,0.95), label = \"Topography of Switzerland\"), family = family, fill = NA, label.color = NA, hjust = 0.5, size = 6, color = fg_color)+\n  ggtext::geom_richtext(aes(x = median(xlims), y = ylims[\"ymin\"], label = \"Data from ©swisstopo<br>visualized by Nils Ratnaweera\"), family = family, fill = NA, label.color = NA, hjust = 0.5, size = 3.5, color = fg_color)+\n  theme_void() +\n  theme(plot.background = element_rect(fill = bg_color,color = NA)) +\n  coord_sf(datum = 21781,xlim =  xlims, ylim = ylims);\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nOriginal (Esp): “Tan linda que duele.”↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "To talk of many things",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\nFederalism at its best: How the open data policy is handled across cantons\n\n\n\n\n\n\nDec 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nWhat changed when switching from Switzerland’s old coordinate system to the new one?\n\n\n\n\n\n\nNov 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nComparing the performance of different methods to do a “point in polygon operation” with sf.\n\n\n\n\n\n\nOct 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA beautiful way to visualize topography, inspired by Carla Martínez Sastre\n\n\n\n\n\n\nAug 13, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching Materials",
    "section": "",
    "text": "An introduction to Spatial R for ArcGIS users  https://arc2r.github.io/book\n\n\n\n\n\n\nAn introduction to GIS in Python for Environmental Scientists (Bachelor Students)  https://modul-agi.github.io/\n\n\n\n\n\n\n\nTeaching R, the tidyverse, data processing, data visualization and statistics  https://researchmethods-zhaw.github.io/\n\n\n\n\n\n\n\nComputational Movement Analysis: Detecting Patterns and Trends in Environmental Data. A course taught by Prof. Dr. Patrick Laube, I was responsible for creating the R-exercises.  https://computationalmovementanalysis.github.io/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "To talk of many things",
    "section": "",
    "text": "My Name is Nils Ratnaweera and I’m a Research Fellow at the Zurich University of Applied Sciences. I’m a Data Scientist who love his job of working with environmental data using mainly open source tools such as R, python, gdal, SQL."
  }
]